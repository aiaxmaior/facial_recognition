{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset Analysis for Video LoRA Training\n",
        "\n",
        "---\n",
        "\n",
        "## ⚠️ CRITICAL INSTRUCTIONS\n",
        "\n",
        "**THE AI ASSISTANT MUST NOT DIRECTLY EVALUATE, INTERPRET, OR VIEW THE CONTENT OF ANY VIDEO/IMAGE DATA.**\n",
        "\n",
        "All processing must be:\n",
        "- Fully automated through scripts\n",
        "- Based on statistical outputs and metadata only\n",
        "- Without human or AI review of actual visual content\n",
        "\n",
        "---\n",
        "\n",
        "## Notebook Sections\n",
        "\n",
        "1. **VLM Captioning** - Generate structured captions via vLLM\n",
        "2. **Emotion Analysis** - Visualize pain/pleasure metrics from preprocessing\n",
        "3. **Category Extraction** - Parse VLM output for character traits and actions\n",
        "4. **Vectorization** - Create sentence embeddings\n",
        "5. **Clustering** - HDBSCAN analysis and visualization\n",
        "6. **Prioritization** - Composite scoring based on primary targets\n",
        "7. **Curation** - Final dataset selection and export"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library\n",
        "import json\n",
        "import sys\n",
        "import re\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "# Data manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# ML / NLP\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import hdbscan\n",
        "import umap\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Progress\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Configure plotting\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"Imports complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path configuration\n",
        "BASE_DIR = Path(\"..\").resolve()\n",
        "ANALYSIS_DIR = BASE_DIR / \"analysis\"\n",
        "CURATED_DIR = BASE_DIR / \"curated\"\n",
        "VLM_DIR = BASE_DIR / \"vlm_copies\"\n",
        "SCENES_DIR = BASE_DIR / \"scenes\"\n",
        "\n",
        "# Ensure output directories exist\n",
        "ANALYSIS_DIR.mkdir(exist_ok=True)\n",
        "CURATED_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Add parent path for imports\n",
        "sys.path.insert(0, str(BASE_DIR.parent))\n",
        "\n",
        "print(f\"Base directory: {BASE_DIR}\")\n",
        "print(f\"Analysis directory: {ANALYSIS_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Preprocessing Results\n",
        "\n",
        "Load the detection and emotion analysis results from preprocessing scripts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load detection results\n",
        "detections_file = ANALYSIS_DIR / \"detections.json\"\n",
        "\n",
        "if detections_file.exists():\n",
        "    with open(detections_file) as f:\n",
        "        detections_data = json.load(f)\n",
        "    print(f\"Loaded {len(detections_data.get('analyses', []))} scene analyses\")\n",
        "    print(f\"Config: {detections_data.get('config', {})}\")\n",
        "    print(f\"Summary: {detections_data.get('summary', {})}\")\n",
        "else:\n",
        "    print(f\"WARNING: Detection file not found: {detections_file}\")\n",
        "    print(\"Run person_detector.py first!\")\n",
        "    detections_data = {'analyses': []}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load emotion results\n",
        "emotions_file = ANALYSIS_DIR / \"emotions.json\"\n",
        "\n",
        "if emotions_file.exists():\n",
        "    with open(emotions_file) as f:\n",
        "        emotions_data = json.load(f)\n",
        "    print(f\"Loaded {len(emotions_data.get('analyses', []))} emotion analyses\")\n",
        "    print(f\"Summary: {emotions_data.get('summary', {})}\")\n",
        "else:\n",
        "    print(f\"WARNING: Emotions file not found: {emotions_file}\")\n",
        "    print(\"Run emotion_detector.py first!\")\n",
        "    emotions_data = {'analyses': []}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create combined DataFrame - merge detection and emotion data by scene path\n",
        "df_detections = pd.DataFrame(detections_data.get('analyses', []))\n",
        "df_emotions = pd.DataFrame(emotions_data.get('analyses', []))\n",
        "\n",
        "if not df_detections.empty and not df_emotions.empty:\n",
        "    # Merge on scene_path\n",
        "    df = pd.merge(\n",
        "        df_detections, \n",
        "        df_emotions,\n",
        "        on='scene_path',\n",
        "        how='left',\n",
        "        suffixes=('_det', '_emo')\n",
        "    )\n",
        "    print(f\"Combined DataFrame: {len(df)} rows\")\n",
        "elif not df_detections.empty:\n",
        "    df = df_detections\n",
        "    print(f\"Using detection data only: {len(df)} rows\")\n",
        "else:\n",
        "    df = pd.DataFrame()\n",
        "    print(\"No data loaded!\")\n",
        "\n",
        "# Filter to scenes with persons\n",
        "if not df.empty and 'person_present' in df.columns:\n",
        "    df_persons = df[df['person_present'] == True].copy()\n",
        "    print(f\"Scenes with persons: {len(df_persons)}\")\n",
        "else:\n",
        "    df_persons = df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Emotion Analysis\n",
        "\n",
        "Visualize pain/pleasure metrics from emotion detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Valence/Arousal Distribution\n",
        "if 'mean_valence' in df_persons.columns:\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "    \n",
        "    # Valence histogram\n",
        "    axes[0].hist(df_persons['mean_valence'].dropna(), bins=30, edgecolor='black', alpha=0.7)\n",
        "    axes[0].axvline(0, color='red', linestyle='--', label='Neutral')\n",
        "    axes[0].set_xlabel('Valence (Pain ← → Pleasure)')\n",
        "    axes[0].set_ylabel('Count')\n",
        "    axes[0].set_title('Valence Distribution')\n",
        "    axes[0].legend()\n",
        "    \n",
        "    # Arousal histogram\n",
        "    axes[1].hist(df_persons['mean_arousal'].dropna(), bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
        "    axes[1].set_xlabel('Arousal (Low ← → High)')\n",
        "    axes[1].set_ylabel('Count')\n",
        "    axes[1].set_title('Arousal Distribution')\n",
        "    \n",
        "    # Valence x Arousal scatter\n",
        "    scatter = axes[2].scatter(\n",
        "        df_persons['mean_valence'], \n",
        "        df_persons['mean_arousal'],\n",
        "        c=df_persons['pain_pleasure_score'],\n",
        "        cmap='RdYlGn',\n",
        "        alpha=0.6,\n",
        "        s=50\n",
        "    )\n",
        "    axes[2].axhline(0.5, color='gray', linestyle=':', alpha=0.5)\n",
        "    axes[2].axvline(0, color='gray', linestyle=':', alpha=0.5)\n",
        "    axes[2].set_xlabel('Valence')\n",
        "    axes[2].set_ylabel('Arousal')\n",
        "    axes[2].set_title('Valence × Arousal Space')\n",
        "    plt.colorbar(scatter, ax=axes[2], label='Pain/Pleasure Score')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(ANALYSIS_DIR / 'valence_arousal_distribution.png', dpi=150)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Emotion data not available. Run emotion_detector.py first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: VLM Captioning\n",
        "\n",
        "Generate structured captions via vLLM (Qwen2.5-VL). This section requires the vLLM server to be running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# VLM Configuration\n",
        "VLLM_URL = \"http://localhost\"\n",
        "VLLM_PORT = 8000\n",
        "\n",
        "# Structured prompt for human interaction analysis\n",
        "VLM_PROMPT = \"\"\"\n",
        "Analyze this video sequence. Provide a structured description:\n",
        "\n",
        "INDIVIDUALS:\n",
        "- Count and describe each person visible (age_category, gender_presentation, distinguishing_features)\n",
        "- Body positions and poses\n",
        "- Clothing/attire\n",
        "\n",
        "INTERACTIONS:\n",
        "- Spatial relationships between individuals\n",
        "- Physical contact type and location (if any)\n",
        "- Eye contact and facial expressions (if visible)\n",
        "- Gesture types\n",
        "\n",
        "ACTIONS:\n",
        "- Primary activity occurring\n",
        "- Secondary/background activities\n",
        "- Motion direction and intensity\n",
        "- Temporal progression (what changes frame to frame)\n",
        "\n",
        "SETTING:\n",
        "- Environment type (indoor/outdoor, room type)\n",
        "- Lighting conditions\n",
        "- Notable objects\n",
        "\n",
        "MOOD/TONE:\n",
        "- Overall emotional atmosphere\n",
        "- Intensity level (calm, active, intense)\n",
        "\n",
        "Output as structured text with clear section headers.\n",
        "\"\"\".strip()\n",
        "\n",
        "print(\"VLM prompt configured\")\n",
        "print(f\"VLM server: {VLLM_URL}:{VLLM_PORT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NOTE: Run this cell only when vLLM server is available\n",
        "# VLM captioning code will be added when server is ready\n",
        "\n",
        "captions = {}\n",
        "captions_file = ANALYSIS_DIR / \"captions.json\"\n",
        "\n",
        "# Load existing captions if available\n",
        "if captions_file.exists():\n",
        "    with open(captions_file) as f:\n",
        "        captions = json.load(f)\n",
        "    print(f\"Loaded {len(captions)} existing captions\")\n",
        "else:\n",
        "    print(\"No existing captions found\")\n",
        "    print(\"Run the VLM captioning cells when vLLM server is available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Clustering & Vectorization\n",
        "\n",
        "Create sentence embeddings and perform HDBSCAN clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load sentence transformer and generate embeddings\n",
        "print(\"Loading sentence transformer model...\")\n",
        "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(f\"Model loaded: {embed_model.get_sentence_embedding_dimension()} dimensions\")\n",
        "\n",
        "# Generate embeddings for captions (if available)\n",
        "if captions:\n",
        "    caption_texts = list(captions.values())\n",
        "    caption_names = list(captions.keys())\n",
        "    \n",
        "    print(f\"Generating embeddings for {len(caption_texts)} captions...\")\n",
        "    embeddings = embed_model.encode(caption_texts, show_progress_bar=True)\n",
        "    print(f\"Embeddings shape: {embeddings.shape}\")\n",
        "    \n",
        "    # Save embeddings\n",
        "    np.save(ANALYSIS_DIR / 'embeddings.npy', embeddings)\n",
        "else:\n",
        "    print(\"No captions to embed - run VLM captioning first\")\n",
        "    embeddings = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HDBSCAN Clustering\n",
        "cluster_labels = None\n",
        "\n",
        "if embeddings is not None and len(embeddings) > 5:\n",
        "    print(\"Running HDBSCAN clustering...\")\n",
        "    \n",
        "    clusterer = hdbscan.HDBSCAN(\n",
        "        min_cluster_size=max(3, len(embeddings) // 10),\n",
        "        min_samples=2,\n",
        "        metric='euclidean',\n",
        "        cluster_selection_method='eom'\n",
        "    )\n",
        "    \n",
        "    cluster_labels = clusterer.fit_predict(embeddings)\n",
        "    \n",
        "    n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
        "    n_noise = (cluster_labels == -1).sum()\n",
        "    \n",
        "    print(f\"Found {n_clusters} clusters\")\n",
        "    print(f\"Noise points: {n_noise}\")\n",
        "    print(f\"Cluster sizes: {Counter(cluster_labels)}\")\n",
        "    \n",
        "    # UMAP for visualization\n",
        "    print(\"\\nRunning UMAP for visualization...\")\n",
        "    reducer = umap.UMAP(n_components=2, n_neighbors=min(15, len(embeddings)-1), \n",
        "                        min_dist=0.1, metric='cosine', random_state=42)\n",
        "    embeddings_2d = reducer.fit_transform(embeddings)\n",
        "    \n",
        "    # Plot\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    scatter = ax.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],\n",
        "                        c=cluster_labels, cmap='tab20', s=100, alpha=0.7,\n",
        "                        edgecolors='black', linewidth=0.5)\n",
        "    ax.set_xlabel('UMAP 1')\n",
        "    ax.set_ylabel('UMAP 2')\n",
        "    ax.set_title('Scene Clusters (HDBSCAN on Caption Embeddings)')\n",
        "    plt.colorbar(scatter, ax=ax, label='Cluster ID')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(ANALYSIS_DIR / 'cluster_visualization.png', dpi=150)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Not enough data for clustering\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Prioritization & Curation\n",
        "\n",
        "Calculate priority scores and select final dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prioritization weights\n",
        "# Pain/Pleasure (35%), Character clarity (30%), Action clarity (25%), Technical quality (10%)\n",
        "WEIGHTS = {'pain_pleasure': 0.35, 'character_clarity': 0.30, \n",
        "           'action_clarity': 0.25, 'technical_quality': 0.10}\n",
        "\n",
        "# Calculate priority scores (based on emotion data primarily)\n",
        "priority_scores = []\n",
        "\n",
        "for idx, row in df_persons.iterrows():\n",
        "    # Pain/Pleasure intensity - high absolute valence * arousal\n",
        "    valence = row.get('mean_valence', 0) or 0\n",
        "    arousal = row.get('mean_arousal', 0.5) or 0.5\n",
        "    pp_score = abs(valence) * arousal\n",
        "    \n",
        "    # Character clarity - detection confidence\n",
        "    confidence = row.get('avg_confidence', 0.5) or 0.5\n",
        "    char_score = confidence\n",
        "    \n",
        "    # Action clarity - based on bbox movement (proxy)\n",
        "    bbox_area = row.get('avg_bbox_area_ratio', 0.2) or 0.2\n",
        "    action_score = min(1.0, bbox_area * 3)\n",
        "    \n",
        "    # Technical quality - detection coverage\n",
        "    coverage = row.get('detection_coverage', 0.5) or 0.5\n",
        "    tech_score = coverage\n",
        "    \n",
        "    # Weighted sum\n",
        "    total = (pp_score * WEIGHTS['pain_pleasure'] + \n",
        "             char_score * WEIGHTS['character_clarity'] +\n",
        "             action_score * WEIGHTS['action_clarity'] + \n",
        "             tech_score * WEIGHTS['technical_quality'])\n",
        "    priority_scores.append(round(total, 4))\n",
        "\n",
        "df_persons['priority_score'] = priority_scores\n",
        "\n",
        "print(f\"Priority scores calculated\")\n",
        "print(f\"Score range: {min(priority_scores):.4f} - {max(priority_scores):.4f}\")\n",
        "print(f\"Mean score: {np.mean(priority_scores):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final dataset curation and export\n",
        "TARGET_SIZE = 150  # Adjust based on available data\n",
        "CORE_RATIO = 0.70  # 70% core, 30% fringe\n",
        "\n",
        "# Select top priority scenes\n",
        "df_sorted = df_persons.sort_values('priority_score', ascending=False)\n",
        "target_size = min(TARGET_SIZE, len(df_sorted))\n",
        "df_curated = df_sorted.head(target_size).copy()\n",
        "\n",
        "print(f\"Curated dataset: {len(df_curated)} scenes\")\n",
        "\n",
        "# Export to LTX-2 format\n",
        "dataset = []\n",
        "for idx, row in df_curated.iterrows():\n",
        "    scene_path = row.get('scene_path', '')\n",
        "    scene_name = Path(scene_path).stem if scene_path else ''\n",
        "    caption = captions.get(scene_name, f\"Scene {scene_name}\")\n",
        "    \n",
        "    dataset.append({\n",
        "        'caption': caption,\n",
        "        'media_path': f\"scenes/{Path(scene_path).name}\"\n",
        "    })\n",
        "\n",
        "# Save dataset\n",
        "dataset_path = CURATED_DIR / 'dataset.json'\n",
        "with open(dataset_path, 'w') as f:\n",
        "    json.dump(dataset, f, indent=2)\n",
        "\n",
        "print(f\"Exported {len(dataset)} entries to {dataset_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Pipeline complete! Output files:\n",
        "- `analysis/detections.json` - Person detection results\n",
        "- `analysis/emotions.json` - Emotion analysis results  \n",
        "- `analysis/captions.json` - VLM captions (when generated)\n",
        "- `analysis/embeddings.npy` - Sentence embeddings\n",
        "- `curated/dataset.json` - LTX-2 compatible dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
